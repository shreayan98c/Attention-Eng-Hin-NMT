{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation Experiments for Hindi-English using an Encoder-Decoder Architecture along with the Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shrea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Installed\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Installed\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Installed\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Installed\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Installed\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Installed\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, RepeatVector, Concatenate, Dot, Lambda\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Input, Model\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127607, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading data\n",
    "data = pd.read_csv('hi_en_corpus.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted</td>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted</td>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                   english_sentence  \\\n",
       "0        ted  politicians do not have permission to do what ...   \n",
       "1        ted         I'd like to tell you about one such child,   \n",
       "2  indic2012  This percentage is even greater than the perce...   \n",
       "3        ted  what we really mean is that they're bad at not...   \n",
       "4  indic2012  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tides        50000\n",
       "ted          39881\n",
       "indic2012    37726\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127607, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting null data from float to string\n",
    "data['english_sentence'] = data['english_sentence'].astype(str)\n",
    "data['hindi_sentence'] = data['hindi_sentence'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n",
      "418\n"
     ]
    }
   ],
   "source": [
    "max_eng_length = max(data['english_sentence'].apply(lambda x: len(x.split(' '))))\n",
    "max_hindi_length = max(data['hindi_sentence'].apply(lambda x: len(x.split(' '))))\n",
    "print(max_eng_length)\n",
    "print(max_hindi_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29786\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "delete_rows = []\n",
    "for i in range(0, data.shape[0]):\n",
    "    len_eng = len(data['english_sentence'][i].split())\n",
    "    len_hin = len(data['hindi_sentence'][i].split())\n",
    "    if len_eng > 25 or len_hin > 25:\n",
    "        delete_rows.append(i)\n",
    "        count = count+1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "english_sentence    0\n",
       "hindi_sentence      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(delete_rows)\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.drop(['source'],axis=1,inplace=True)\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95086, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_english_data(sentence):\n",
    "    exclude = set(string.punctuation)\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(ch for ch in sentence if ch not in exclude)\n",
    "    sentence = sentence.translate(remove_digits)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(\" +\", \" \", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hindi_data(sentence):\n",
    "    exclude = set(string.punctuation)\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(ch for ch in sentence if ch not in exclude)\n",
    "\n",
    "    sent_temp = ''\n",
    "    for c in sentence:\n",
    "        if c == ' ':\n",
    "            sent_temp += c\n",
    "        elif ord(u'\\u0900') <= ord(c) <= ord(u'\\u097F'):\n",
    "            sent_temp += c\n",
    "    sentence = sent_temp\n",
    "\n",
    "    sentence = re.sub('[a-z]', '', sentence)\n",
    "    sentence = re.sub('[०१२३४५६७८९]', '', sentence)\n",
    "    sentence = sentence.translate(remove_digits)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(\" +\", \" \", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [clean_english_data(x) for x in data['english_sentence'].values]\n",
    "Y = [clean_hindi_data(y) for y in data['hindi_sentence'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a part of this process we'll tokenize both the input and output. The important thing over here is the addition of the <START>(START) and <END>(END) tags on the output sequence. This is for our neural network to understand the sequence and when to break the processing in our architecture. Then we will padd the tokens to the largest length of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x,is_hindi = False):\n",
    "    # Add START and END tag to the sentence\n",
    "    tokenizer_input = Tokenizer(num_words=MAX_VOCAB)\n",
    "    if is_hindi:\n",
    "        target_input_sequences = []\n",
    "        target_sequences = []\n",
    "        for sentence in x:\n",
    "            target_input_sequences.append('START_ '+sentence)\n",
    "            target_sequences.append(sentence+' _END')\n",
    "        tokenizer_output = Tokenizer(num_words=30000)\n",
    "        tokenizer_output.fit_on_texts(target_input_sequences+target_sequences)\n",
    "        target_input_sequences = tokenizer_output.texts_to_sequences(target_input_sequences)\n",
    "        target_sequences = tokenizer_output.texts_to_sequences(target_sequences)\n",
    "        return target_input_sequences,target_sequences,tokenizer_output\n",
    "    else:\n",
    "        tokenizer_input = Tokenizer(num_words=30000) \n",
    "        tokenizer_input.fit_on_texts(x)\n",
    "        input_sequences = tokenizer_input.texts_to_sequences(x)\n",
    "        return input_sequences,tokenizer_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95086 95086\n"
     ]
    }
   ],
   "source": [
    "input_sequences, tokenizer_input = tokenize(X)\n",
    "target_input_sequences,target_sequences, tokenizer_output = tokenize(Y,is_hindi=True)\n",
    "print(len(target_input_sequences),len(target_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    padded_x = pad_sequences(x, maxlen = length, padding = 'post', truncating = 'post')\n",
    "    return padded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = pad(input_sequences)\n",
    "target_input_sequences = pad(target_input_sequences)\n",
    "target_sequences = pad(target_sequences)\n",
    "\n",
    "input_num_words = len(tokenizer_input.word_index) + 1\n",
    "target_num_words = len(tokenizer_output.word_index) + 1  \n",
    "\n",
    "max_input_len = len(input_sequences[0])\n",
    "max_target_len = len(target_sequences[0])\n",
    "\n",
    "words_input = tokenizer_input.word_index\n",
    "words_output = tokenizer_output.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size:  49253\n",
      "Hindi vocab size:  48413\n"
     ]
    }
   ],
   "source": [
    "print(\"English vocab size: \", input_num_words)\n",
    "print(\"Hindi vocab size: \", target_num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is prepared, we can proceed by creating the model architecture\n",
    "\n",
    "We will create the input and the target word embedding matrix.\n",
    "\n",
    "We will be working with Bi-Directional LSTM as we need to make sure that the context of the output is maintained along with the sequence structure. This is the core advantage of using BiLSTM as it not only preserver information and data from past (like single unit LSTM) but also considers future data. \n",
    "\n",
    "The output structure will be closesly related to the context of input which is very essential while converting sentences from one language to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSIONS = 100\n",
    "LSTM_UNITS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting word to word vectors\n",
    "word2Vec = {}\n",
    "with open('C:/Users/shrea/Desktop/Jupyter Notebooks/IITB Internship/Russian Translation/Experiments/Pretrained/glove.6B.100d.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        line = line.split(' ')\n",
    "        word = line[0]\n",
    "        word2Vec[word] = line[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embedding_matrix = np.zeros((input_num_words, DIMENSIONS))\n",
    "for word, k in words_input.items():\n",
    "    if k < input_num_words:\n",
    "        embedding_vector = word2Vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            input_embedding_matrix[k] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embedding_matrix = np.zeros((target_num_words, DIMENSIONS))\n",
    "for word, k in words_input.items():\n",
    "    if k < target_num_words:\n",
    "        embedding_vector = word2Vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            target_embedding_matrix[k] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Installed\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Creating the input layer embeddings\n",
    "# We take the 2D input and convert it to a tensor shaped embedding matrix.\n",
    "embedding_input = Input(shape=(max_input_len,))\n",
    "embedding_input_layer = Embedding(input_num_words, DIMENSIONS, weights=[input_embedding_matrix], trainable=True)\n",
    "x = embedding_input_layer(embedding_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Neural Machine Translation, it is been found that deeper architectures tends to have much better performance than a single unit neural network. In [this paper](https://www.aclweb.org/anthology/W17-4710.pdf) the author goes on to explain why deeper architectures are much useful for NMT and goes on to suggest that variation of DeepBi-RNN with depth 8 performs the best. To keep the computational and understanding simpler, we will restrict ourselves to 2 units of Bi-LSTM to improve the performance over single unit Neural Nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lstm1 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))\n",
    "input_lstm1_output = input_lstm1(x)\n",
    "\n",
    "input_lstm2 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))\n",
    "encoder_output = input_lstm2(input_lstm1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_over_time(x):\n",
    "    assert(K.ndim(x) > 2)\n",
    "    e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "    s = K.sum(e, axis=1, keepdims=True)\n",
    "    return e / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is needed so that the long sequences of the textual data can be processed with maximum precision. Repeat vector is used to repeat the same input vectors at every iteration with different hidden state. We will use the tanh and softmax activation functions for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "atten_layer_repeat = RepeatVector(max_input_len)\n",
    "atten_concatenate = Concatenate(axis=-1)\n",
    "atten_dense1 = Dense(30, activation='tanh')\n",
    "atten_dense2 = Dense(1, activation=softmax_over_time)\n",
    "attn_dot = Dot(axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_procedure(h, st_1):\n",
    "    st_1 = atten_layer_repeat(st_1)\n",
    "    x = atten_concatenate([h, st_1])\n",
    "    x = atten_dense1(x)\n",
    "    alphas = atten_dense2(x)\n",
    "    context = attn_dot([alphas,h])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_0 = Input(shape=(LSTM_UNITS,))\n",
    "c_0 = Input(shape=(LSTM_UNITS,))\n",
    "context_last_word_concat_layer = Concatenate(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding embedding layer at output and Initiating the Model. We feed the output of the first LSTM layer to input the second LSTM layer before finally applying the softmax activation function on the output. If we feed the softmax to the output of first LSTM unit, it won't give us great results.\n",
    "\n",
    "We also stack the tensors to get the output and feed it to the model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_decoder_input = Input(shape=(max_target_len,))\n",
    "embedding_decoder_layer = Embedding(target_num_words, DIMENSIONS, weights=[target_embedding_matrix], trainable=True)\n",
    "decoder_x = embedding_decoder_layer(embedding_decoder_input)\n",
    "\n",
    "s = st_0\n",
    "c = c_0\n",
    "outputs = []\n",
    "decoder_lstm = LSTM(LSTM_UNITS, return_state=True)\n",
    "decoder_dense_layer = Dense(target_num_words, activation='softmax')\n",
    "for i in range(max_target_len):\n",
    "    context = attention_procedure(encoder_output, s)\n",
    "    selector = Lambda(lambda x: x[:, i:i+1])\n",
    "    xt = selector(decoder_x)\n",
    "    decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
    "\n",
    "    decoder_lstm_output, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s,c])\n",
    "\n",
    "    decoder_output = decoder_dense_layer(decoder_lstm_output)\n",
    "    outputs.append(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_and_transpose(x):\n",
    "    x = K.stack(x) \n",
    "    x = K.permute_dimensions(x, pattern=(1, 0, 2)) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)\n",
    "\n",
    "model = Model(\n",
    "  inputs=[\n",
    "    embedding_input,\n",
    "    embedding_decoder_input,\n",
    "    st_0, \n",
    "    c_0,\n",
    "  ],\n",
    "  outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared the data and the model architecture to train the model. We will use callback functions to regularly save our progress and then train our model.\n",
    "\n",
    "We have used sparse_categorical_crossentropy as we have taken the target to be integers and not one-hot vectors. [This blog article](https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046) gives us an idea as to how to how to fine tune the hyperparameters. For our project, rms_prop proved to be a better optimizer than Adam and hence, accepted in our training model.\n",
    "\n",
    "We split train-test 80-20 and then train our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"C:/Users/shrea/Desktop/Jupyter Notebooks/IITB Internship/Russian Translation/Experiments/Attention-Eng-Hin-NMT/Checkpoints/weights-{epoch:02d}-{val_acc:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "from keras.models import load_model\n",
    "\n",
    "model.load_weights('C:/Users/shrea/Desktop/Jupyter Notebooks/IITB Internship/Russian Translation/Experiments/Attention-Eng-Hin-NMT/Checkpoints/weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "z = np.zeros((len(target_sequences), LSTM_UNITS))\n",
    "r = model.fit(\n",
    "  [input_sequences, target_input_sequences, z, z], target_sequences.reshape(target_sequences.shape[0],target_sequences.shape[1], 1),\n",
    "  batch_size=64,\n",
    "  epochs=10,\n",
    "  validation_split=0.2,\n",
    "  verbose=1,\n",
    "  callbacks=callbacks_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

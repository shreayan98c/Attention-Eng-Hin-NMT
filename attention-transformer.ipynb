{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BtQ7yQmXHSF"
   },
   "source": [
    "# Neural Machine Translation Experiments for Hindi-English using an Encoder-Decoder Architecture along with the Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "VoCYqBhY0LBa",
    "outputId": "adeaa2c7-1817-4f24-983b-3480e6fcf78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, RepeatVector, Concatenate, Dot, Lambda\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Input, Model\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dL5ZiunA0ToJ"
   },
   "outputs": [],
   "source": [
    "# Reading data\n",
    "df = pd.read_excel('gdrive/My Drive/Hi-En-Parallel_Corpus.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "rDFxdgQH0bR_",
    "outputId": "098d5fd8-36b1-4c46-beb8-a933b4e0d3de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence                                     hindi_sentence\n",
       "0  politicians do not have permission to do what ...  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...\n",
       "1         I'd like to tell you about one such child,  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...\n",
       "2  This percentage is even greater than the perce...   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।\n",
       "3  what we really mean is that they're bad at not...     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते\n",
       "4  .The ending portion of these Vedas is called U...        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fBwQBHeMawOm"
   },
   "source": [
    "On exploring the dataset, I found a lot of irregularities. Some data items were missing while some only contained integer/float values. This makes the operations for our task very cumbersome and hence, we need to convert all the items to string data type. \n",
    "\n",
    "Also, next part is to find the length of longest hindi and english sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Bt5y_1V0bcMt",
    "outputId": "b682a4f0-1fca-41c1-d295-1a1e9f0697e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127607, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['english_sentence'] = df['english_sentence'].astype(str)\n",
    "df['hindi_sentence'] = df['hindi_sentence'].astype(str)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "zQvhu_u5bhQ1",
    "outputId": "09b7c22e-805f-496a-974d-c0f1fa841e22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n",
      "418\n"
     ]
    }
   ],
   "source": [
    "max_eng_length = max(df['english_sentence'].apply(lambda x: len(x.split(' '))))\n",
    "max_hindi_length = max(df['hindi_sentence'].apply(lambda x: len(x.split(' '))))\n",
    "print(max_eng_length)\n",
    "print(max_hindi_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRvl-ouIc_7I"
   },
   "source": [
    "As you can see that without any pre-processing, our length of the sentences in both the columns are really huge. \n",
    "The 1.2L data items need to be reduced to make our model less computationally expensive and without compromising the accuracy. \n",
    "\n",
    "For this project, I have tried to bring the training parameters to 75K to achieve the best accuracy with the available GPU.\n",
    "To decide which items to be dropped, we'll find all the rows having vocab length greater than 25. \n",
    "\n",
    "We'll also apply few more tools to clean the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DCjTP9Yt0e4-",
    "outputId": "5d8f79b4-4394-4718-b8a8-c5330dd2a923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29786\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "delete_rows = []\n",
    "for i in range(0,df.shape[0]):\n",
    "    len_eng = len(df['english_sentence'][i].split())\n",
    "    len_hin = len(df['hindi_sentence'][i].split())\n",
    "    if len_eng > 25 or len_hin > 25:\n",
    "        delete_rows.append(i)\n",
    "        count = count +1 \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a7wIl--x0jou",
    "outputId": "ef12c035-7412-4ba7-8daa-5aebcfd9a4be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97821, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(delete_rows)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "neJ_t_fD0oGk",
    "outputId": "8f3ed4ee-e771-4567-faa5-f4756e7dfb59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95080, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-BDIcV8eadv"
   },
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "As a part of this module we'll work separately on english and hindi sentences. \n",
    "We'll be pre-processing to do the following tasks:\n",
    "\n",
    "\n",
    "*   Removing numbers and digits from both input and output data\n",
    "*   Removing Whitespaces in the sentence (leading and trailing)\n",
    "*   Converting the entire sentence to lowercase\n",
    "*   Remove all kinds of punctiontion items like ',{&%$' from the sentence. \n",
    "*   Remove english words in the Hindi sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH5nhnyl0p_a"
   },
   "outputs": [],
   "source": [
    "def clean_english_data(sentence):\n",
    "    exclude = set(string.punctuation)\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(ch for ch in sentence if ch not in exclude)\n",
    "    sentence = sentence.translate(remove_digits)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(\" +\", \" \", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xria-56h0sRq"
   },
   "outputs": [],
   "source": [
    "def clean_hindi_data(sentence):\n",
    "    exclude = set(string.punctuation)\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(ch for ch in sentence if ch not in exclude)\n",
    "\n",
    "    sent_temp = ''\n",
    "    for c in sentence:\n",
    "        if c == ' ':\n",
    "            sent_temp += c\n",
    "        elif ord(u'\\u0900') <= ord(c) <= ord(u'\\u097F'):\n",
    "            sent_temp += c\n",
    "    sentence = sent_temp\n",
    "      \n",
    "    sentence = re.sub('[a-z]', '', sentence)\n",
    "    sentence = re.sub('[०१२३४५६७८९।]', '', sentence)\n",
    "    sentence = sentence.translate(remove_digits)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(\" +\", \" \", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNeJXwSf0uuZ"
   },
   "outputs": [],
   "source": [
    "X = [clean_english_data(x) for x in df.english_sentence.values]\n",
    "Y = [clean_hindi_data(y) for y in df.hindi_sentence.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_zcUUUy0y5b"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LKVvSOFhO4H"
   },
   "source": [
    "## Tokenization and Padding\n",
    "\n",
    "As a part of this process we'll tokenize both the input and output. The important thing over here is the addition of the `<START_>`(START) and` <_END>`(END) tags on the output sequence. This is for our neural network to understand the sequence and when to break the processing in our architecture. \n",
    "\n",
    "Next step is to padd the tokens to the largest length of the sentence(which we have set to 25). We find that we get hindi length size to be 26(24+2). It's 24 because of removal of whitespaces in the previous step or else ideally it should have been 27(25+2). \n",
    "\n",
    "We also find the vocab size of our input and output data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xvPsj8O00-w"
   },
   "outputs": [],
   "source": [
    "def tokenize(x,is_hindi = False):\n",
    "    # Add START_ and END_ to the sentence\n",
    "    tokenizer_input = Tokenizer(num_words=MAX_VOCAB)\n",
    "    if is_hindi:\n",
    "        target_input_sequences = []\n",
    "        target_sequences = []\n",
    "        for sentence in x:\n",
    "            target_input_sequences.append('START_ '+sentence)\n",
    "            target_sequences.append(sentence+' _END')\n",
    "        tokenizer_output = Tokenizer(num_words=30000)\n",
    "        tokenizer_output.fit_on_texts(target_input_sequences+target_sequences)\n",
    "        target_input_sequences = tokenizer_output.texts_to_sequences(target_input_sequences)\n",
    "        target_sequences = tokenizer_output.texts_to_sequences(target_sequences)\n",
    "        return target_input_sequences,target_sequences,tokenizer_output\n",
    "    else:\n",
    "        tokenizer_input = Tokenizer(num_words=30000) \n",
    "        tokenizer_input.fit_on_texts(x)\n",
    "        input_sequences = tokenizer_input.texts_to_sequences(x)\n",
    "        return input_sequences,tokenizer_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I0IEIwAF1mK-",
    "outputId": "5c483cf8-ce70-4583-b656-20b7fc5dea5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95080 and  95080\n"
     ]
    }
   ],
   "source": [
    "input_sequences, tokenizer_input = tokenize(X)\n",
    "target_input_sequences,target_sequences, tokenizer_output = tokenize(Y,is_hindi=True)\n",
    "print(len(target_input_sequences),\"and \",len(target_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PP93lfBE3rvW"
   },
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    print(\"The max length is \",length)    \n",
    "    padded_x = pad_sequences(x, maxlen = length, padding = 'post', truncating = 'post')\n",
    "    return padded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "N08dscvA5SQE",
    "outputId": "c5940cd7-b9cc-4605-fd8e-7e2da10cebcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length is  25\n",
      "The max length is  26\n",
      "The max length is  26\n",
      "English vocab size:  49253\n",
      "Hindi vocab size:  47529\n",
      "\n",
      "Length of longest English sentence:  25\n",
      "Length of longest Hindi sentence:  26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sequences = pad(input_sequences)\n",
    "target_input_sequences = pad(target_input_sequences)\n",
    "target_sequences = pad(target_sequences)\n",
    "\n",
    "input_num_words = len(tokenizer_input.word_index) + 1\n",
    "target_num_words = len(tokenizer_output.word_index) + 1  \n",
    "\n",
    "print(\"English vocab size: \", input_num_words)\n",
    "print(\"Hindi vocab size: \", target_num_words)\n",
    "print()\n",
    "  \n",
    "max_input_len = len(input_sequences[0])\n",
    "max_target_len = len(target_sequences[0])\n",
    "\n",
    "words_input = tokenizer_input.word_index\n",
    "words_output = tokenizer_output.word_index\n",
    "\n",
    "print(\"Length of longest English sentence: \", max_input_len)\n",
    "print(\"Length of longest Hindi sentence: \", max_target_len)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GcALuxs78Z_O"
   },
   "source": [
    "## Model Architecture\n",
    "\n",
    "Now let's dive into the most important part of our project. \n",
    "\n",
    "We initially create input and target word embedding matrix. We have chose glove file to be 6B.100d as 100 dimensions makes it easier and giving us approximately the same accuracy as 300d. \n",
    "\n",
    "We will be working with **Bi-Directional LSTM** as we need to make sure that the context of the output is maintained along with the sequence structure. This is the core advantage of using BiLSTM as it not only preserver information and data from past(like single unit LSTM) but also considers future data. The output structure will be closesly related to the context of input which is very essential while converting sentences from one language to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1hNfzBx8bud"
   },
   "outputs": [],
   "source": [
    "DIMENSIONS = 100\n",
    "LSTM_UNITS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y126mR4Q858n"
   },
   "outputs": [],
   "source": [
    "word2Vec = {}\n",
    "with open('gdrive/My Drive/glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.split(' ')\n",
    "        word = line[0]\n",
    "        word2Vec[word] = line[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCcckygE6w8p"
   },
   "outputs": [],
   "source": [
    "input_embedding_matrix = np.zeros((input_num_words, DIMENSIONS))\n",
    "for word, k in words_input.items():\n",
    "    if k < input_num_words:\n",
    "        embedding_vector = word2Vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            input_embedding_matrix[k] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAjpHWCT69nu"
   },
   "outputs": [],
   "source": [
    "target_embedding_matrix = np.zeros((target_num_words, DIMENSIONS))\n",
    "for word, k in words_input.items():\n",
    "    if k < target_num_words:\n",
    "        embedding_vector = word2Vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            target_embedding_matrix[k] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tb7GH1JxCLxn"
   },
   "source": [
    "Let's start creating our embedding input layer. We already have all the values set. We take the 2D input and convert it to a tensor shaped embedding matrix. In [7] we have everything that we wish to know about embedding layers and its importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8chhC309Tz8"
   },
   "outputs": [],
   "source": [
    "embedding_input = Input(shape=(max_input_len,))\n",
    "embedding_input_layer = Embedding(input_num_words, DIMENSIONS, weights=[input_embedding_matrix], trainable=True)\n",
    "x = embedding_input_layer(embedding_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PnOytTa7EQpI"
   },
   "source": [
    "In Neural Machine Translation, it is been found that deeper architectures tends to have much better performance than a single unit neural network. In [8] the author goes on to explain why deeper architectures are much useful for NMT and goes on to suggest that variation of DeepBi-RNN with depth 8 performs the best. \n",
    "To keep the computational and understanding simpler, we will restrict ourselves to 2 units of Bi-LSTM to improve the performance over single unit Neural Nets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1E0i-ET9XdX"
   },
   "outputs": [],
   "source": [
    "input_lstm1 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))\n",
    "input_lstm1_output = input_lstm1(x)\n",
    "\n",
    "input_lstm2 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))\n",
    "encoder_output = input_lstm2(input_lstm1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q5Gd2fgm9jaq"
   },
   "outputs": [],
   "source": [
    "def softmax_over_time(x):\n",
    "    assert(K.ndim(x) > 2)\n",
    "    e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "    s = K.sum(e, axis=1, keepdims=True)\n",
    "    return e / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ck-056qXF7g_"
   },
   "source": [
    "## Attention\n",
    "\n",
    "Attention is needed so that the long sequences of the textual data can be processed with maximum precision. Repeat vector is used to repeat the same input vectors at every iteration with different hidden state. [6] and [9] gives us a complete theorectical background as to why the specific tanh and softmax functions are used in the process. Taken the help of [10] to code the attention on keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCLQmIma9y5v"
   },
   "outputs": [],
   "source": [
    "atten_layer_repeat = RepeatVector(max_input_len)\n",
    "atten_concatenate = Concatenate(axis=-1)\n",
    "atten_dense1 = Dense(30, activation='tanh')\n",
    "atten_dense2 = Dense(1, activation=softmax_over_time)\n",
    "attn_dot = Dot(axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Szc3yIv090wJ"
   },
   "outputs": [],
   "source": [
    "def attention_procedure(h, st_1):\n",
    "    st_1 = atten_layer_repeat(st_1)\n",
    "    x = atten_concatenate([h, st_1])\n",
    "    x = atten_dense1(x)\n",
    "    alphas = atten_dense2(x)\n",
    "    context = attn_dot([alphas,h])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1nkoIvA926J"
   },
   "outputs": [],
   "source": [
    "st_0 = Input(shape=(LSTM_UNITS,))\n",
    "c_0 = Input(shape=(LSTM_UNITS,))\n",
    "context_last_word_concat_layer = Concatenate(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7RQoryJmkRU"
   },
   "source": [
    "Decoding embedding layer at outpuut and Initiating the Model.\n",
    "we feed the output of the first LSTM layer to input the second LSTM layer before finally applying the softmax activation function on the output. If we feed the softmax to the output of first LSTM unit, it won't give us great results. \n",
    "\n",
    "We also stack the tensors to get the output and feed it to the model function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgWFwpvq95-g"
   },
   "outputs": [],
   "source": [
    "embedding_decoder_input = Input(shape=(max_target_len,))\n",
    "embedding_decoder_layer = Embedding(target_num_words, DIMENSIONS, weights=[target_embedding_matrix], trainable=True)\n",
    "decoder_x = embedding_decoder_layer(embedding_decoder_input)\n",
    "\n",
    "s = st_0\n",
    "c = c_0\n",
    "outputs = []\n",
    "decoder_lstm = LSTM(LSTM_UNITS, return_state=True)\n",
    "decoder_dense_layer = Dense(target_num_words, activation='softmax')\n",
    "for i in range(max_target_len):\n",
    "    context = attention_procedure(encoder_output, s)\n",
    "\n",
    "    selector = Lambda(lambda x: x[:, i:i+1])\n",
    "    xt = selector(decoder_x)\n",
    "    decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
    "\n",
    "    decoder_lstm_output, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s,c])\n",
    "\n",
    "    decoder_output = decoder_dense_layer(decoder_lstm_output)\n",
    "    outputs.append(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gj7iXVsD99fG"
   },
   "outputs": [],
   "source": [
    "def stack_and_transpose(x):\n",
    "    x = K.stack(x) \n",
    "    x = K.permute_dimensions(x, pattern=(1, 0, 2)) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHwwdEMV-HjP"
   },
   "outputs": [],
   "source": [
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)\n",
    "\n",
    "model = Model(\n",
    "  inputs=[\n",
    "    embedding_input,\n",
    "    embedding_decoder_input,\n",
    "    st_0, \n",
    "    c_0,\n",
    "  ],\n",
    "  outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAvLwNAhsFB-"
   },
   "source": [
    "## Model Training\n",
    "\n",
    "Finally it's time to train the network we created in the above steps. We make the callback functions to regularly save our progress and then train our LSTM model.\n",
    "\n",
    "We have used sparse_categorical_crossentropy as we have taken the target to be integers and not one-hot vectors. [11] gives us an idea as to how to how to fine tune the hyperparameters. For our project, rms_prop proved to be a better optimizer than Adam and hence, accepted in our training model. \n",
    "\n",
    "We split train-test 80-20 and then train our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YlokO3GC-KRr"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"/content/gdrive/My Drive/Checkpoints/weights-{epoch:02d}-{val_acc:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nV9UmeYaSuS"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "from keras.models import load_model\n",
    "\n",
    "model.load_weights('/content/gdrive/My Drive/Checkpoints/weights-11-0.692.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "colab_type": "code",
    "id": "FWo50_Ch-jo5",
    "outputId": "fc4e321b-810f-4c96-bc6b-c7fb3199205b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76064 samples, validate on 19016 samples\n",
      "Epoch 1/10\n",
      "76064/76064 [==============================] - 329s 4ms/step - loss: 1.6693 - acc: 0.7346 - val_loss: 2.0387 - val_acc: 0.6928\n",
      "\n",
      "Epoch 00001: saving model to /content/gdrive/My Drive/Checkpoints/weights-01-0.693.hdf5\n",
      "Epoch 2/10\n",
      "76064/76064 [==============================] - 318s 4ms/step - loss: 1.6059 - acc: 0.7439 - val_loss: 2.0437 - val_acc: 0.6938\n",
      "\n",
      "Epoch 00002: saving model to /content/gdrive/My Drive/Checkpoints/weights-02-0.694.hdf5\n",
      "Epoch 3/10\n",
      "76064/76064 [==============================] - 317s 4ms/step - loss: 1.5524 - acc: 0.7521 - val_loss: 2.0397 - val_acc: 0.6945\n",
      "\n",
      "Epoch 00003: saving model to /content/gdrive/My Drive/Checkpoints/weights-03-0.694.hdf5\n",
      "Epoch 4/10\n",
      "76064/76064 [==============================] - 316s 4ms/step - loss: 1.4932 - acc: 0.7608 - val_loss: 2.0416 - val_acc: 0.6956\n",
      "\n",
      "Epoch 00004: saving model to /content/gdrive/My Drive/Checkpoints/weights-04-0.696.hdf5\n",
      "Epoch 5/10\n",
      "76064/76064 [==============================] - 316s 4ms/step - loss: 1.4477 - acc: 0.7684 - val_loss: 2.0554 - val_acc: 0.6950\n",
      "\n",
      "Epoch 00005: saving model to /content/gdrive/My Drive/Checkpoints/weights-05-0.695.hdf5\n",
      "Epoch 6/10\n",
      "76064/76064 [==============================] - 317s 4ms/step - loss: 1.4071 - acc: 0.7757 - val_loss: 2.0709 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00006: saving model to /content/gdrive/My Drive/Checkpoints/weights-06-0.694.hdf5\n",
      "Epoch 7/10\n",
      "76064/76064 [==============================] - 320s 4ms/step - loss: 1.3693 - acc: 0.7826 - val_loss: 2.0838 - val_acc: 0.6934\n",
      "\n",
      "Epoch 00007: saving model to /content/gdrive/My Drive/Checkpoints/weights-07-0.693.hdf5\n",
      "Epoch 8/10\n",
      "76064/76064 [==============================] - 320s 4ms/step - loss: 1.3290 - acc: 0.7892 - val_loss: 2.0944 - val_acc: 0.6941\n",
      "\n",
      "Epoch 00008: saving model to /content/gdrive/My Drive/Checkpoints/weights-08-0.694.hdf5\n",
      "Epoch 9/10\n",
      "76064/76064 [==============================] - 319s 4ms/step - loss: 1.2892 - acc: 0.7959 - val_loss: 2.1111 - val_acc: 0.6931\n",
      "\n",
      "Epoch 00009: saving model to /content/gdrive/My Drive/Checkpoints/weights-09-0.693.hdf5\n",
      "Epoch 10/10\n",
      "76064/76064 [==============================] - 318s 4ms/step - loss: 1.2507 - acc: 0.8021 - val_loss: 2.1255 - val_acc: 0.6940\n",
      "\n",
      "Epoch 00010: saving model to /content/gdrive/My Drive/Checkpoints/weights-10-0.694.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "z = np.zeros((len(target_sequences), LSTM_UNITS))\n",
    "r = model.fit(\n",
    "  [input_sequences, target_input_sequences, z, z], target_sequences.reshape(target_sequences.shape[0],target_sequences.shape[1], 1),\n",
    "  batch_size=128,\n",
    "  epochs=10,\n",
    "  validation_split=0.2,\n",
    "  verbose=1,\n",
    "  callbacks=callbacks_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZX75rWRFt2q_"
   },
   "source": [
    "As you can see above that we achieved a training accuracy of 80.21 % and validation accuracy of **69.4%** for our dataset. We trained it for 20 epochs (weight till 10th epoch loaded from the latest checkpoint and then trained for another 10 epochs) to achieve to this accuracy and as you can see there is no scope for improvement in the future epochs, we conclude our training stage. \n",
    "\n",
    "Below is a small visualization of our training model for the last 10 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "Ch1LMAyT-qMD",
    "outputId": "4fe0dd0a-c99e-4224-b871-1c6eb3d112d4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(r.history['acc'])\n",
    "plt.plot(r.history['val_acc'])\n",
    "plt.title('Training and Validation Accuracy',size = 20)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iuS0i3mJ8ZWo"
   },
   "outputs": [],
   "source": [
    "model.save('/content/gdrive/My Drive/Checkpoints/translator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeMM56AjKpFX"
   },
   "outputs": [],
   "source": [
    "model.save_weights('/content/gdrive/My Drive/Checkpoints/translator1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01wu5aEVvEvV"
   },
   "source": [
    "**Encoder and Decoder**\n",
    "\n",
    "Finally we test our model and initiate our encoder and decoder functions. We make the predictions by giving the data to the encoder and receiving the output at decoder. \n",
    "\n",
    "A step wise end-end process is being carried out for our neural architecture and translation is performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3sJQvb98jsX"
   },
   "outputs": [],
   "source": [
    "encoder_outputs_as_input = Input(shape=(max_input_len, LSTM_UNITS * 2,))\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = embedding_decoder_layer(decoder_inputs_single)\n",
    "\n",
    "context = attention_procedure(encoder_outputs_as_input, st_0)\n",
    "\n",
    "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
    "\n",
    "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[st_0, c_0])\n",
    "decoder_outputs = decoder_dense_layer(o)\n",
    "\n",
    "decoder_model = Model(\n",
    "  inputs=[\n",
    "    decoder_inputs_single,\n",
    "    encoder_outputs_as_input,\n",
    "    st_0, \n",
    "    c_0\n",
    "  ],\n",
    "  outputs=[decoder_outputs, s, c]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JF3k3jrIxmw"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(embedding_input, encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aUaDZ536Iza4"
   },
   "outputs": [],
   "source": [
    "idx2word = {v:k for k, v in words_output.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VegyfXp5I08y"
   },
   "outputs": [],
   "source": [
    "def prediction(input_seq):\n",
    "\n",
    "    enc_out = encoder_model.predict([[input_seq]])\n",
    "\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "\n",
    "    target_seq[0, 0] = words_output['START_']\n",
    "\n",
    "    eos = words_output['_END']\n",
    "\n",
    "    s = np.zeros((1, LSTM_UNITS))\n",
    "    c = np.zeros((1, LSTM_UNITS))\n",
    "\n",
    "\n",
    "    output_sentence = []\n",
    "    for _ in range(max_target_len):\n",
    "        o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
    "\n",
    "\n",
    "        idx = np.argmax(o.flatten())\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "    target_seq[0, 0] = idx\n",
    "    \n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cs6K0ZBCI5MX"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "predictions_inputs = []\n",
    "true_outputs = []\n",
    "count = 0\n",
    "for i in range(0,5):\n",
    "    predictions_inputs.append(df['english_sentence'][i])\n",
    "    true_outputs.append(df['hindi_sentence'][i])\n",
    "    count += 1\n",
    "predictions_inputs = tokenizer_input.texts_to_sequences(predictions_inputs)\n",
    "predictions_inputs = pad_sequences(predictions_inputs, maxlen=max_input_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R80-C10tv5bu"
   },
   "source": [
    "## Final Translation\n",
    "\n",
    "Finally we are able to test our NMT architecture. I have provided few samples below. Before that a bit of processing is needed to segregate our actual and predicted outputs. \n",
    "\n",
    "On careful observation of various samples, the model seems to be performing well for short sentences. For long sentences, even if the grammatical structure is not completely accurate, it still retains the context of the input. The results are encouraging and can be improved in the ways explained towards the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qID6AUUASR1r",
    "outputId": "0eb3fa55-e1bb-4f14-d860-75684a37da64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I started my journey in California\n",
      "मैंने अपना सफ़र कैलिफोर्निया से शुरू किया\n"
     ]
    }
   ],
   "source": [
    "true_outputs = []\n",
    "true_inputs = []\n",
    "df2 = df['hindi_sentence'].tail(n=19016).tolist()\n",
    "df3 = df['english_sentence'].tail(n=19016).tolist()\n",
    "true_outputs.extend(df2)\n",
    "true_inputs.extend(df3)\n",
    "print(true_inputs[0])\n",
    "print(true_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Ke0dL6wZX2l_",
    "outputId": "bb705707-df2a-4ae4-9d62-12181b72300d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  16  159   64 1247    5 3342    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "test_input = input_sequences[-19016:]\n",
    "print(test_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text = []\n",
    "count = 0\n",
    "for i in test_input:\n",
    "    predicted_output = prediction(i)\n",
    "    print(count)\n",
    "    count = count + 1\n",
    "    predicted_text.append(predicted_output)\n",
    "print(predicted_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1lbe4gqXcOIa"
   },
   "outputs": [],
   "source": [
    "with open('/content/gdrive/My Drive/output.txt', 'w') as f:\n",
    "    for row in predicted_text:\n",
    "        f.write(str(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "7YxoFzVZhDDS",
    "outputId": "c9b3f8da-d8be-4912-bb89-55b8f55845f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence is\n",
      "Its main entrance is from Lahore gate.\n",
      "---------------------\n",
      "Actual Hindi Sentence\n",
      "लाहौर गेट इसका मुख्य प्रवेशद्वार है।\n",
      "---------------------\n",
      "Predicted Hindi Sentence\n",
      "इसका मुख्य लाहौर गेट प्रधान लाहौर से किमी है\n",
      "English Sentence is\n",
      "in other essays collection of different views,thoughts,pictured,speeches far sight are important.\n",
      "---------------------\n",
      "Actual Hindi Sentence\n",
      "अन्य निबंध में संकल्पिता तथा विविध संकलनों में स्मारिका स्मृति चित्र संभाषण संचयन दृष्टिबोध उल्लेखनीय हैं।\n",
      "---------------------\n",
      "Predicted Hindi Sentence\n",
      "अन्य विभिन्न स्थानों पर विभिन्न प्रकार की सामाजिक दृष्टि से उनका महत्वपूर्ण है\n",
      "English Sentence is\n",
      "Hanuman is an ideal devotee, and is always ready to assist Rama and serve him.\n",
      "---------------------\n",
      "Actual Hindi Sentence\n",
      "हनुमान एक आदर्श भक्त हैं वे राम की सेवा के लिये अनुचर के समान सदैव तत्पर रहते हैं।\n",
      "---------------------\n",
      "Predicted Hindi Sentence\n",
      "हनुमान को आदर्श में भगवान् जाने के लिए हनुमान हैं और रावण का वध करते हैं\n",
      "English Sentence is\n",
      "I started my journey in California\n",
      "---------------------\n",
      "Actual Hindi Sentence\n",
      "मैंने अपना सफ़र कैलिफोर्निया से शुरू किया\n",
      "---------------------\n",
      "Predicted Hindi Sentence\n",
      "मैंने यह साल की में यात्रा किया\n",
      "English Sentence is\n",
      "When we talk of anti-imperialism , most of us think it is against England .\n",
      "---------------------\n",
      "Actual Hindi Sentence\n",
      "जब हम साम्राज़्यवाद के खिलाफ कुछ कहते हैं , तब बहुत-से लोग यह समझते हैं कि हम इंग़्लैंड के खिलाफ बात करते हैं .\n",
      "---------------------\n",
      "Predicted Hindi Sentence\n",
      "जब हम हम जानते हैं कि हम बारे में कि हम सोचते हैं कि उनका खिलाफ है\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(0,5):\n",
    "    random_integer = random.randint(0,100)\n",
    "    print('English Sentence is')\n",
    "    print(true_inputs[random_integer])\n",
    "    print('---------------------')\n",
    "    print('Actual Hindi Sentence')\n",
    "    print(true_outputs[random_integer])\n",
    "    print('---------------------')\n",
    "    print('Predicted Hindi Sentence')\n",
    "    print(predicted_text[random_integer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "roylu_OVxeGj"
   },
   "source": [
    "## Model Evaluation \n",
    "\n",
    "Finally we evaluate the performance of our model using BLEU metric. We evaluate the performance for the actual and predicted sentences upto 4-gram and giving us an accuracy of **0.75** and an average of **0.59.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DOwpopUHLoeH"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "def evaluate_model(actual,predicted):\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "7Iw02dsAb-yE",
    "outputId": "2d1d5917-6396-4697-a0e7-89112d805a3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.324522\n",
      "BLEU-2: 0.569668\n",
      "BLEU-3: 0.713466\n",
      "BLEU-4: 0.754764\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(true_outputs,predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6z2Vm3nkyP8l"
   },
   "source": [
    "## Future Scope and Tasks ahead\n",
    "\n",
    "The above work gives us satisfactory results but still lacks on few aspects. It's important to atleast consider them for our evaluation so that those points could be improved in future. A majority of them lies in the pre-processing and handling of the data.\n",
    "\n",
    "*   Hindi Tokenizer can be improved greatly if we use an external library for that. The pre-processing and tokenizing of hindi sentences can be done much more effecently by using the Indic-nlp library : https://github.com/anoopkunchukuttan/indic_nlp_library\n",
    "*   A lot of sentences had dates and years which were removed during the pre-processing which broke the context of then sentences. Work can be done to make the dates machine readable(Since, the instructions specified to remove digits, this part was not possible).\n",
    "*   The hyperparameters can still be fine tuned by vigorous testing to improve the performance. \n",
    "*   GPU constraints allowed us to train on only 76K parameters. Full use of the dataset might improve the performance. \n",
    "*   Accuracy for long sentences is still not satisfactory and needs a better way to handle it. A possible approach is to reduce the batch size and increase the deep layer architecture. \n",
    "*   Shifting the architecture to Pytorch framework might improve the overall results as it has a better attention mechansim than keras.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCQrej6bDBU6"
   },
   "source": [
    "## References\n",
    "\n",
    "1.   https://towardsdatascience.com/neural-machine-translation-15ecf6b0b\n",
    "2.   https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "3.   https://pravn.wordpress.com/2017/11/14/bahdanau-attention/\n",
    "4.   https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/\n",
    "5.   https://towardsdatascience.com/implementing-neural-machine-translation-with-attention-using-tensorflow-fc9c6f26155f\n",
    "6. https://medium.com/@joealato/attention-in-nlp-734c6fa9d983\n",
    "7.   https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "8. https://www.aclweb.org/anthology/W17-4710.pdf\n",
    "9. https://blog.floydhub.com/attention-mechanism/\n",
    "10. https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39\n",
    "11. https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NMT-Final.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
